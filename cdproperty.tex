\section{Correlation Decay Property}

In the last section we show an algorithm $P(G,e,L)$ for estimating the marginal probability $P(G,e)$,
so here we establish the exponential correlation decay property, in the stronger sense with the $M$-based depth, of the estimation error in $P(G,e,L)$.%, hence $P(G,e,L)$

\begin{Thm}
	Given graph $G$, edge $e$ and depth $L$,
	\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}\]
\end{Thm}

Such phenomenon is usually refered to as exponential correlation decay. Before we prove the main theorem, we will introduce a few useful propositions and lemmas.

\begin{Prop}
	\[P(G, e) \leq \frac{1}{2}\]
\end{Prop}

\begin{proof}
	Although one may examine this case by case algebraically, this propositions can be seen quite obvious combinatorially in that, for any edge cover $X \in EC(G)$ s.t. $e \notin X$, $X+e$ is also an edge cover in $G$, and $\forall X,Y \in EC(G)$ s.t. $X \neq Y, e \notin X, e\notin Y$, we have $X+e \neq Y+e$. So the edge covers with $e$ chosen is at least as many as the edge covers with $e$ not chosen, hence concludes the proposition follows.
\end{proof}

For notational convenience, given a d-dimensional vector $x \in [0, \frac{1}{2}]^d$, we denote
\[ f(x) \triangleq \frac{1- \prod_i x_i}{2 - \prod_i x_i}\]

Given a $d_1$-dimensional vector $x \in [0, \frac{1}{2}]^{d_1}$ and two $d_2$-dimensional vectors $y,z \in [0, \frac{1}{2}]^{d_2}$, let
\[ g(x,y,z) \triangleq  1- \frac{1}{2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i} \]


	\begin{Lem}
		\begin{align*}
			\abs{\sum_i \cfrac{\partial f(x)}{\partial x_i}} \leq \frac{1}{2}. 
		\end{align*}
	\end{Lem}

	\begin{proof}
		Denote $i^*$ be one of the indices of smallest $x_i$, since $x_i \leq \frac{1}{2}$, we have
	\begin{align*}
		\abs{\sum_i \cfrac{\partial f(x)}{\partial x_i}}  =& \cfrac{\sum_i \prod_{k \neq i} x_k  }{\left( 2 - \prod_i x_i \right)^2} \\
		\leq & d \prod_{k \neq i^*} x_k \\
		\leq & d \left( \frac{1}{2} \right)^{d-1}
	\end{align*}

	So for $d \geq 4$ we have $\abs{\sum_i \cfrac{\partial f(x)}{\partial x_i}} \leq \frac{1}{2}$.

	For $d=0, \abs{\sum_i \cfrac{\partial f(x)}{\partial x_i}} = 0$.

	Now consider $d=1$, $\abs{\sum_i \cfrac{\partial f(x)}{\partial x_i}} = \frac{1}{\left( 2 - x_1 \right)^2} \leq \frac{4}{9} $.

	Next consider $d=2$,  $\abs{\sum_i \cfrac{\partial f(x)}{\partial x_i}} = \frac{x_1 + x_2}{\left( 2 - x_1x_2 \right)^2} \leq \frac{16}{49} $.

	Finally for $d=3$,  $\abs{\sum_i \cfrac{\partial f(x)}{\partial x_i}} = \frac{x_1 + x_2 + x_3}{\left( 2 - x_1x_2x_3 \right)^2} \leq \frac{16}{75} $.
	\end{proof}


	\begin{Lem}
		\begin{align*}
			\abs{ \sum_i \frac{\partial g(x,y,z)}{\partial x_i} } \leq & 1 \\
			\abs{ \sum_i \frac{\partial g(x,y,z)}{\partial y_i} } \leq & 1 \\
			\abs{ \sum_i \frac{\partial g(x,y,z)}{\partial z_i} } \leq & 1 
		\end{align*}
	\end{Lem}

	\begin{proof}
		\begin{align*}
		\abs{ \sum_i \frac{\partial g(x,y,z)}{\partial x_i} } &= \sum_i \frac{\prod_{k\neq i} x_k \left( 1 - \prod_k y_k \right)}{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2} \\
		&\leq d_1 \frac{1}{2^{d_1 - 1}}  \leq 1 \\
		\abs{ \sum_i \frac{\partial g(x,y,z)}{\partial y_i} } &= \sum_i \frac{\prod_{k\neq i} x_k \left( 1 - \prod_k y_k \right)}{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2} \\
		&\leq d_2 \frac{1}{2^{d_1 + d_2 - 1}} \leq 1 \\
		\abs{ \sum_i \frac{\partial g(x,y,z)}{\partial z_i} } &= \sum_i \frac{\prod_{k\neq i} x_k \left( 1 - \prod_k y_k \right)}{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2} \\
		&\leq d_2 \frac{1}{2^{d_2 - 1}} \leq 1 \\
		\end{align*}
	\end{proof}

	Now we are ready for the main theorem.

	\begin{proof}
		First we note that by Mean Value Theorem,
		given estimated $\hat{x}$, let $x$ be the true value, let $i^*$ be the indices that maximizes $\abs{x_i - \hat{x_i}}$.
		we have
		\[ \abs{f(\hat{x}) - f(x)} \leq \abs{\sum_i \frac{\partial f(x)}{\partial x_i}} \cdot \abs{x_i^* - \hat{x_i}^*} \]

		Given estimated $\hat{x},\hat{y},\hat{z}$,
		\[ \abs{g(\hat{x}, \hat{y}, \hat{z}) - g(x,y,z)} \leq \abs{\sum_i \frac{\partial f(x)}{\partial x_i}} \cdot \abs{x_i^* - \hat{x_i}^*} \]
		\[ \abs{g(\hat{x}, \hat{y}, \hat{z}) - g(x,y,z)} \leq \abs{\sum_i \frac{\partial f(x)}{\partial x_i}} \cdot \abs{x_i^* - \hat{x_i}^*} \]
		\[ \abs{g(\hat{x}, \hat{y}, \hat{z}) - g(x,y,z)} \leq \abs{\sum_i \frac{\partial f(x)}{\partial x_i}} \cdot \abs{x_i^* - \hat{x_i}^*} \]


		We prove by simple induction.$L=0, \abs{P(G,e,L) - P(G,e)} \leq \frac{1}{2}$ holds when $e$ is free or dangling, when $e$ is normal we have 

		Case 1, $e$ is dangling edge.
	\end{proof}<++>
	Note that the recursion for general graph is applied only once, so it's sufficient to show that the sum of the partial derivatives is bounded.
