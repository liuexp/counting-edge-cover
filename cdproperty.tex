\section{Correlation Decay Property}

In the last section, we show an algorithm $P(G,e,L)$ for estimating the marginal probability $P(G,e)$,
so here we establish the exponential correlation decay property, in the stronger sense with the
%modified recursion depth,
$M$-based depth,
of the estimation error in $P(G,e,L)$.%, hence $P(G,e,L)$

\begin{Thm}
	\label{cd-main-theorem}
	Given graph $G$, edge $e$ and depth $L$,
	\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}\]
\end{Thm}

Such phenomenon is usually referred to as exponential correlation decay. Before we prove the main theorem, we will introduce a few useful propositions and lemmas.

\begin{Prop}
	\[P(G, e) \leq \frac{1}{2}\]
\end{Prop}

\begin{proof}
	Although one may examine this case by case algebraically, this proposition is quite obvious in a combinatorial view, for any edge cover $X \in EC(G)$ with $e \notin X$, $X+e$ is also an edge cover in $G$, and $\forall X,Y \in EC(G)$ s.t. $X \neq Y, e \notin X, e\notin Y$, we have $X+e \neq Y+e$. So the edge covers with $e$ chosen is at least as many as the edge covers with $e$ not chosen, hence the proposition follows.
\end{proof}

We remark that our algorithm also guarantees that $P(G,e,L) \leq \frac{1}{2}$,
since for the dangling case, $\frac{1 - \prod_i x_i}{2 - \prod_i x_i} = \frac{1}{2} - \frac{\prod_i x_i}{2(2 - \prod_i x_i)}$ ; and for normal case $X\cdot Y - X - Z \leq 0$.

For notational convenience, given a d-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^d$, we denote
\[ f({\bf x}) \triangleq \frac{1- \prod_i x_i}{2 - \prod_i x_i}\]

Given a $d_1$-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^{d_1}$ and two $d_2$-dimensional vectors ${\bf y,z} \in [0, \frac{1}{2}]^{d_2}$, let
\[ g({\bf x,y,z}) \triangleq  1- \frac{1}{2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i} \]


	\begin{Lem}
		\label{meanvalue1}
		For $d$-variate function $f$, given estimated $\hat{\bf x}$ for true value ${\bf x}$ such that $\hat{\bf x} \in  [0, \frac{1}{2}]^d, {\bf x} \in [0, \frac{1}{2}]^d$, let $\epsilon \triangleq \max_i{\abs{x_i - \hat{x_i}}}$,
		\begin{align*}
			\abs{f(\hat{\bf x}) - f({\bf x})}
		\leq  \min{\set{\frac{1}{2}, d \left( \frac{1}{2} \right)^{d-1}}} \cdot \epsilon
		\end{align*}
	\end{Lem}

	\begin{proof}
        First we show for any d-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^d$,
        \begin{align}
			\label{mv1pd}
			\sum_i^d \abs{\cfrac{\partial f({\bf x})}{\partial x_i}} \leq & \min{\set{\frac{1}{2}, d \left( \frac{1}{2} \right)^{d-1}}}
		\end{align}

    	For $d=0, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = 0$.

    	For $d=1, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{1}{\left( 2 - x_1 \right)^2} \leq \frac{4}{9} $.

    	For $d=2, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{x_1 + x_2}{\left( 2 - x_1x_2 \right)^2} \leq \frac{16}{49} $.

    	For $d=3, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{x_1 x_2 + x_1 x_3 + x_2 x_3}{\left( 2 - x_1x_2x_3 \right)^2} \leq \frac{16}{75} $.

		Now denote $i^*$ be one of the indices of smallest $x_i$, since $x_i \leq \frac{1}{2}$, we have
    	\begin{align*}
    		\sum_i^d \abs{\cfrac{\partial f({\bf x})}{\partial x_i}}  =& \sum_i^d \cfrac{ \prod_{k \neq i}^d x_k  }{\left( 2 - \prod_i x_i \right)^2}
    		\leq  d \prod_{k \neq i^*}^d x_k
    		\leq  d \left( \frac{1}{2} \right)^{d-1}
    	\end{align*}

		So for $d \geq 4, \sum_i \abs{\cfrac{\partial f({\bf x})}{\partial x_i}} \leq \frac{1}{2}$, we have verified the inequality relation (\ref{mv1pd}).

        Now let $g(\alpha) = f(\alpha {\bf x} + (1 - \alpha) {\bf \hat{x}})$ where $\alpha \in [0,1]$, by (\ref{mv1pd}) and Mean Value Theorem, $\exists \tilde{\alpha} \in [0,1]$ s.t. for $\tilde{\bf x} = \tilde{\alpha} {\bf x} + (1 - \tilde{\alpha}) {\bf \hat{x}}$

		\begin{align*}
			\abs{f(\hat{\bf x}) - f({\bf x})} \leq& \sum_i\abs{ \frac{\partial f(\tilde{\bf x})}{\partial x_i}} \cdot \epsilon
		\leq  \min{\set{\frac{1}{2}, d \left( \frac{1}{2} \right)^{d-1}}} \cdot \epsilon
		\end{align*}
	\end{proof}

	\begin{Lem}
		\label{meanvalue2}
		Given estimated $\hat{\bf x},\hat{\bf y},\hat{\bf z}$ for true value $ {\bf x}, {\bf y}, {\bf z}$ respectively, such that ${\bf x} \in [0, \frac{1}{2}]^{d_1} , {\bf y,z} \in [0, \frac{1}{2}]^{d_2}, \hat{\bf x} \in [0, \frac{1}{2}]^{d_1} , \hat{\bf y},\hat{\bf z} \in [0, \frac{1}{2}]^{d_2}$,
		let $\epsilon \triangleq \max_i \set{\abs{x_i - \hat{x_i}}, \abs{y_i - \hat{y_i} } , \abs{z_i - \hat{z_i}}}$,
		\begin{align*}
		\abs{g(\hat{\bf x}, \hat{\bf y}, \hat{\bf z}) - g({\bf x,y,z})}
		\leq  3\epsilon
		\end{align*}

	\end{Lem}

	\begin{proof}
		First for any ${\bf x} \in [0, \frac{1}{2}]^{d_1} , {\bf y,z} \in [0, \frac{1}{2}]^{d_2}$,
		\begin{align*}
		\sum_k^{d_1} \abs{ \frac{\partial g({\bf x,y,z})}{\partial x_k} } &= \sum_k^{d_1} \frac{\prod_{i\neq k}^{d_1} x_i \cdot \left( 1 - \prod_i^{d_2} y_i \right)}{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2}
		\leq d_1\cdot  \frac{1}{2^{d_1 - 1}}  \leq 1 \\
		\sum_k^{d_2} \abs{ \frac{\partial g({\bf x,y,z})}{\partial y_k} } &= \sum_k^{d_2} \frac{\prod_{i\neq k}^{d_2} y_i \cdot \prod_{i}^{d_1} x_i}{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2}
		\leq d_2 \cdot \frac{1}{2^{d_1 + d_2 - 1}} \leq 1 \\
		\sum_k^{d_2} \abs{ \frac{\partial g({\bf x,y,z})}{\partial z_k} } &= \sum_k^{d_2} \frac{\prod_{i\neq k}^{d_2} z_i }{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2}
		\leq d_2 \cdot \frac{1}{2^{d_2 - 1}} \leq 1
		\end{align*}

		Then by Mean Value Theorem, $\exists \tilde{\bf x}, \tilde{\bf y},  \tilde{\bf z}$ s.t. $\forall i, \min\set{x_i,\hat{x_i}} \leq \tilde{x_i} \leq \max\set{x_i, \hat{x_i}}, \min\set{y_i,\hat{y_i}} \leq \tilde{y_i} \leq \max\set{y_i, \hat{y_i}}, \min\set{z_i,\hat{z_i}} \leq \tilde{z_i} \leq \max\set{z_i, \hat{z_i}}$.
		\begin{align*}
		\abs{g(\hat{\bf x}, \hat{\bf y}, \hat{\bf z}) - g({\bf x,y,z})} \leq &
		\left(  \sum_i\abs{ \frac{\partial g(\tilde{\bf x},\tilde{\bf y},\tilde{\bf z} )}{\partial x_i}} + \sum_i\abs{ \frac{\partial g(\tilde{\bf x},\tilde{\bf y},\tilde{\bf z} )}{\partial y_i}} +  \sum_i\abs{ \frac{\partial g(\tilde{\bf x},\tilde{\bf y},\tilde{\bf z} )}{\partial z_i}} \right)  \epsilon
		\leq  3\epsilon
		\end{align*}
	\end{proof}

	%Now we are ready for the proof of  Theorem \ref{cd-main-theorem}.

	%\begin{proof}
\noindent{\bf Proof of Theorem \ref{cd-main-theorem}. } 
		Note that the recursion for normal edge case is applied only once, so it is sufficient to show that:

		\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}, \textrm{for free or dangling edge $e$}\]
		
		And it automatically follows from Lemma \ref{meanvalue2} that
		\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}, \textrm{for normal edge $e$}\]

		Now we prove by induction with induction hypothesis:
		\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}, \textrm{for free or dangling edge $e$}\]
		
		For base case $L=0, \abs{P(G,e,L) - P(G,e)} \leq \frac{1}{2}$ holds when $e$ is free or dangling.

		Now suppose the induction hypothesis is true for $L<k$, we shall prove that it is true for $L=k$.

		{\bf Case 1}, $e$ is free edge, then $\abs{P(G,e,L) - P(G,e)} = 0$.

		{\bf Case 2}, $e=(u,\_)$ is dangling with $deg(u)=d+1$, then by induction hypothesis we have
		$\epsilon \triangleq \abs{P\left(G,e,L-\lceil \log_6{d+1}\rceil \right) - P(G,e)} \leq \left(\frac{1}{2}\right)^{L-\lceil \log_6{d+1}\rceil}$.

		First by Lemma \ref{meanvalue1} we need to show that for $d \leq 4$,
		\[\frac{1}{2^{1+L-\lceil \log_6{(d+1)}\rceil}} \leq \frac{1}{2^L}\]

		which is obvious because $\lceil\log_6{(d+1)}\rceil \leq 1$.

		Next we show for $d \geq 5$,
		\[ d\cdot \left( \frac{1}{2} \right)^{d-1 + L - \lceil \log_6{(d+1)}\rceil}  \leq \left( \frac{1}{2} \right)^L \]

		Namely for $d \geq 5$,
		\[ \log_2 d + \lceil \log_6{(d+1)} \rceil \leq d-1\]

		For $d=5,6$, one can directly examine that as $\log_2 d < 3$ and $\log_6 6 =1, \log_6 7 < 2$.

        %For $d\geq7$, by simply taking the derivative one can show that
		%\[ \log_2 d + \log_6{(d+1)} \leq d-1\]
		% @mrain:The derivative is wrong.
%        Since $\frac{d \log_2 x + \log_6 (x+1)}{dx} = \frac{1}{x} + \frac{1}{x+1} < 1$ when $x \geq 7$.
%        So for $d \ge 7$,

		For $d\geq 7$, since the function $f(x) = d-2 -\log_2 d - \log_6{(d+1)}$ is monotonically increasing, and $f(7)>0$, we have
        \[ \log_2 d + \log_6{(d+1)} + 1 \leq d-1\]

%		$\epsilon \leq \frac{1}{2}^{L - \lceil d/5 \rceil}$.
%		First we need to show that for $d \leq 5$,
%		\[\frac{1}{2^{1+L-\lceil d/5\rceil}} \leq \frac{1}{2^L}\]
%
%		which is obvious because $\lceil d/5 \rceil \leq 1$,
%
%		Next we show for $d \geq 6$,
%		\[ d\cdot \left( \frac{1}{2} \right)^{d-1 + L - \lceil d/5 \rceil}  \leq \left( \frac{1}{2} \right)^L \]
%
%		Namely for $d \geq 6$,
%		\[ \log_2 d + \lceil d/5 \rceil \leq d-1\]
%
%		This is followed from that the function $f(x) = x - 2 - d/5 - \log_2 d$ is monotonically increasing, and $f(6)>0$.

		Therefore, the hypothesis for $L=k$ is verified.
		%\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}, \textrm{for free or dangling edge $e$}\]

		To sum up, the case of free or dangling edge and the case of normal edge together conclude the proof for our main theorem.
	%\end{proof}
