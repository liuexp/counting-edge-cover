\section{Correlation Decay Property}

In the last section we show an algorithm $P(G,e,L)$ for estimating the marginal probability $P(G,e)$,
so here we establish the exponential correlation decay property, in the stronger sense with the 
%modified recursion depth, 
$M$-based depth,
of the estimation error in $P(G,e,L)$.%, hence $P(G,e,L)$

\begin{Thm}
	\label{cd-main-theorem}
	Given graph $G$, edge $e$ and depth $L$,
	\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}\]
\end{Thm}

Such phenomenon is usually refered to as exponential correlation decay. Before we prove the main theorem, we will introduce a few useful propositions and lemmas.

\begin{Prop}
	\[P(G, e) \leq \frac{1}{2}\]
\end{Prop}

\begin{proof}
	Although one may examine this case by case algebraically, this propositions can be seen quite obvious combinatorially in that, for any edge cover $X \in EC(G)$ s.t. $e \notin X$, $X+e$ is also an edge cover in $G$, and $\forall X,Y \in EC(G)$ s.t. $X \neq Y, e \notin X, e\notin Y$, we have $X+e \neq Y+e$. So the edge covers with $e$ chosen is at least as many as the edge covers with $e$ not chosen, hence the proposition follows.
\end{proof}

We remark that our algorithm also guarantees that $P(G,e,L) \leq \frac{1}{2}$, since $\frac{1 - \prod_i x_i}{2 - \prod_i x_i} = \frac{1}{2} - \frac{\prod_i x_i}{2(2 - \prod_i x_i)}$, and $X\cdot Y - X - Z \leq 0$.

For notational convenience, given a d-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^d$, we denote
\[ f({\bf x}) \triangleq \frac{1- \prod_i x_i}{2 - \prod_i x_i}\]

Given a $d_1$-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^{d_1}$ and two $d_2$-dimensional vectors ${\bf y,z} \in [0, \frac{1}{2}]^{d_2}$, let
\[ g({\bf x,y,z}) \triangleq  1- \frac{1}{2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i} \]


	\begin{Lem}
		For $d$-variate function $f({\bf x})$,and a d-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^d$,
		\begin{align*}
			\sum_i^d \abs{\cfrac{\partial f({\bf x})}{\partial x_i}} \leq & \min{\set{\frac{1}{2}, d \left( \frac{1}{2} \right)^{d-1}}}
		\end{align*}
	\end{Lem}

	\begin{proof}
		Denote $i^*$ be one of the indices of smallest $x_i$, since $x_i \leq \frac{1}{2}$, we have
	\begin{align*}
		\sum_i^d \abs{\cfrac{\partial f({\bf x})}{\partial x_i}}  =& \sum_i^d \cfrac{ \prod_{k \neq i}^d x_k  }{\left( 2 - \prod_i x_i \right)^2} \\
		\leq & d \prod_{k \neq i^*}^d x_k \\
		\leq & d \left( \frac{1}{2} \right)^{d-1}
	\end{align*}

	So for $d \geq 4$ we have $\sum_i \abs{\cfrac{\partial f({\bf x})}{\partial x_i}} \leq \frac{1}{2}$.

	For $d=0, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = 0$.

	Now consider $d=1$, $\sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{1}{\left( 2 - x_1 \right)^2} \leq \frac{4}{9} $.

	Next consider $d=2$,  $\sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{x_1 + x_2}{\left( 2 - x_1x_2 \right)^2} \leq \frac{16}{49} $.

	Finally for $d=3$,  $\sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{x_1 + x_2 + x_3}{\left( 2 - x_1x_2x_3 \right)^2} \leq \frac{32}{75} $.
	\end{proof}


	\begin{Lem}
Given a $d_1$-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^{d_1}$ and two $d_2$-dimensional vectors ${\bf y,z} \in [0, \frac{1}{2}]^{d_2}$,
\label{lemnormalpd}
		\begin{align*}
			 \sum_i\abs{ \frac{\partial g({\bf x,y,z})}{\partial x_i} } \leq & 1 \\
			\sum_i \abs{ \frac{\partial g({\bf x,y,z})}{\partial y_i} } \leq & 1 \\
			\sum_i \abs{ \frac{\partial g({\bf x,y,z})}{\partial z_i} } \leq & 1
		\end{align*}
	\end{Lem}

	\begin{proof}
		\begin{align*}
			\sum_k^{d_1} \abs{ \frac{\partial g({\bf x,y,z})}{\partial x_k} } &= \sum_k^{d_1} \frac{\prod_{i\neq k}^{d_1} x_i \cdot \left( 1 - \prod_i^{d_2} y_i \right)}{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2} 
		\leq d_1\cdot  \frac{1}{2^{d_1 - 1}}  \leq 1 \\
		\sum_k^{d_2} \abs{ \frac{\partial g({\bf x,y,z})}{\partial y_k} } &= \sum_k^{d_2} \frac{\prod_{i\neq k}^{d_2} y_i \cdot \prod_{i}^{d_1} x_i}{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2} 
		\leq d_2 \cdot \frac{1}{2^{d_1 + d_2 - 1}} \leq 1 \\
		\sum_k^{d_2} \abs{ \frac{\partial g({\bf x,y,z})}{\partial z_k} } &= \sum_k^{d_2} \frac{\prod_{i\neq k}^{d_2} z_i }{(2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i)^2} 
		\leq d_2 \cdot \frac{1}{2^{d_2 - 1}} \leq 1 \\
		\end{align*}
	\end{proof}

	Now we are ready for the main theorem.

	\begin{proof}
		First we note that by Mean Value Theorem,
		given estimated $\hat{\bf x}$, let ${\bf x}$ be the true value, let $\epsilon = \max_i{\abs{x_i - \hat{x_i}}}$.
		we have for $d$-variate function $f$,

		\begin{align}
		\abs{f(\hat{\bf x}) - f({\bf x})} \leq& \sum_i\abs{ \frac{\partial f({\bf x})}{\partial x_i}} \cdot \epsilon 
		\leq  d\cdot \left( \frac{1}{2} \right)^{d-1} \cdot \epsilon
		\label{meanvalue1}
		\end{align}

		Given estimated $\hat{\bf x},\hat{\bf y},\hat{\bf z}$, let $\epsilon = \max \set{\abs{x_i - \hat{x_i}}, \abs{y_i - \hat{y_i} } , \abs{z_i - \hat{z_i}}}$.
		\begin{align}
		\abs{g(\hat{\bf x}, \hat{\bf y}, \hat{\bf z}) - g({\bf x,y,z})} \leq & \left(  \sum_i\abs{ \frac{\partial g({\bf x,y,z})}{\partial x_i}} + \sum_i\abs{ \frac{\partial g({\bf x,y,z})}{\partial y_i}} +  \sum_i\abs{ \frac{\partial g({\bf x,y,z})}{\partial z_i}} \right)  \epsilon 
		\leq  3\epsilon
		\label{meanvalue2}
		\end{align}


		Also note that the recursion for normal edge case is applied only once, so it's sufficient to show for free or dangling edge $e$,
		\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}, \textrm{for free or dangling edge $e$}\]
		then the case of normal edge automatically follows from (\ref{meanvalue2}) that
		\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}, \textrm{for normal edge $e$}\]

		Now we prove by induction with induction hypothesis:
		%\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}, \textrm{for normal edge $e$}\]
		\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}, \textrm{for free or dangling edge $e$}\]
		
		For base case $L=0, \abs{P(G,e,L) - P(G,e)} \leq \frac{1}{2}$ holds when $e$ is free or dangling.% When $e$ is normal, since the normal case only appears once, we have the maximal estimation error for the first two cases that $\epsilon \leq \frac{1}{2}$, so to sum up we have $\abs{P(G,e,L) - P(G,e)} \leq 3\cdot\frac{1}{2}$.

		Now suppose for $L<k$ we have the induction hypothesis true, now we try to show it's true for $L=k$.

		{\bf Case 1}, $e$ is free edge, then $\abs{P(G,e,L) - P(G,e)} = 0$.

		{\bf Case 2}, $e=(u,\_)$ is dangling with $deg(u)=d+1$, then by induction hypothesis we have 
		$\epsilon \leq \frac{1}{2}^{L-\lceil \log_6{d+1}\rceil}$.

		First we need to show that for $d \leq 4$,
		\[\frac{1}{2^{1+L-\lceil \log_6{(d+1)}\rceil}} \leq \frac{1}{2^L}\]

		which is obvious because $\lceil\log_6{(d+1)}\rceil \leq 1$.

		Next we show for $d \geq 5$,
		\[ d\cdot \left( \frac{1}{2} \right)^{d-1 + L - \lceil \log_6{(d+1)}\rceil}  \leq \left( \frac{1}{2} \right)^L \]

		Namely for $d \geq 5$,
		\[ \log_2 d + \lceil \log_6{(d+1)} \rceil \leq d-1\]

		For $d=5,6$, one can directly examine that as $\log_2 d < 3$ and $\log_6 6 =1, \log_6 7 < 2$.

        %For $d\geq7$, by simply taking the derivative one can show that
		%\[ \log_2 d + \log_6{(d+1)} \leq d-1\]
		% @mrain:The derivative is wrong.
%        Since $\frac{d \log_2 x + \log_6 (x+1)}{dx} = \frac{1}{x} + \frac{1}{x+1} < 1$ when $x \geq 7$.
%        So for $d \ge 7$,

		For $d\geq 7$, note that the function $f(x) = d-2 -\log_2 d - \log_6{(d+1)}$ is monotonically increasing, and $f(7)>0$, so we have 
        \[ \log_2 d + \log_6{(d+1)} + 1 \leq d-1\]

%		$\epsilon \leq \frac{1}{2}^{L - \lceil d/5 \rceil}$.
%		First we need to show that for $d \leq 5$,
%		\[\frac{1}{2^{1+L-\lceil d/5\rceil}} \leq \frac{1}{2^L}\]
%
%		which is obvious because $\lceil d/5 \rceil \leq 1$, 
%
%		Next we show for $d \geq 6$,
%		\[ d\cdot \left( \frac{1}{2} \right)^{d-1 + L - \lceil d/5 \rceil}  \leq \left( \frac{1}{2} \right)^L \]
%
%		Namely for $d \geq 6$,
%		\[ \log_2 d + \lceil d/5 \rceil \leq d-1\]
%
%		This is followed from that the function $f(x) = x - 2 - d/5 - \log_2 d$ is monotonically increasing, and $f(6)>0$.

		Therefore, the hypothesis for $L=k$ is verified.
		\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}, \textrm{for free or dangling edge $e$}\]

		%{\bf Case 3}, $e$ is normal edge. Since the normal case only appears once at the root of the computation tree,
		%by induction hypothesis we have the maximal estimation error $\epsilon \leq (\frac{1}{2})^{L+1}$,
		%now we have the first part of the hypothesis for $L=k$ verified.
		%\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}, \textrm{for normal edge $e$}\]
		
		To sum up, the case of free or dangling edge and the case of normal edge together conclude the proof for our main theorem.
	\end{proof}
