\section{Correlation Decay Property}

In the last section, we show an algorithm $P(G,e,L)$ for estimating the marginal probability $P(G,e)$,
so here we establish the exponential correlation decay property, in the stronger sense with the
%modified recursion depth,
$M$-based depth,
of the estimation error in $P(G,e,L)$.%, hence $P(G,e,L)$

\begin{theorem}
	\label{cd-main-theorem}
	Given graph $G$, edge $e$ and depth $L$,
	\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}\]
\end{theorem}

Such phenomenon is usually referred to as exponential correlation decay. Before we prove the main theorem, we will introduce a few useful propositions and lemmas.

\begin{proposition}
	\[P(G, e) \leq \frac{1}{2}\]
\end{proposition}

\begin{proof}
	Although one may examine this case by case algebraically, this proposition is quite obvious in a combinatorial view, for any edge cover $X \in EC(G)$ with $e \notin X$, $X+e$ is also an edge cover in $G$, and $\forall X,Y \in EC(G)$ s.t. $X \neq Y, e \notin X, e\notin Y$, we have $X+e \neq Y+e$. So the edge covers with $e$ chosen is at least as many as the edge covers with $e$ not chosen, hence the proposition follows.
\end{proof}

We remark that our algorithm also guarantees that $P(G,e,L) \leq \frac{1}{2}$,
since for the dangling case, $\frac{1 - \prod_i x_i}{2 - \prod_i x_i} = \frac{1}{2} - \frac{\prod_i x_i}{2(2 - \prod_i x_i)}$ ; and for normal case $X\cdot Y - X - Z \leq 0$.

For notational convenience, given $d$-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^d$, we denote
\[ f({\bf x}) \triangleq \frac{1- \prod_i x_i}{2 - \prod_i x_i}\]

Given a $d_1$-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^{d_1}$ and two $d_2$-dimensional vectors ${\bf y,z} \in [0, \frac{1}{2}]^{d_2}$, let
\[ g({\bf x,y,z}) \triangleq  1- \frac{1}{2+\prod_i x_i \cdot \prod_i y_i - \prod_i x_i - \prod_i z_i} \]


	\begin{lemma}
		\label{meanvalue1}
		For $d$-variate function $f$, given estimated $\hat{\bf x}$ for true value ${\bf x}$ such that $\hat{\bf x} \in  [0, \frac{1}{2}]^d, {\bf x} \in [0, \frac{1}{2}]^d$, let $\epsilon \triangleq \max_i{\abs{x_i - \hat{x_i}}}$,
		\begin{align*}
			\abs{f(\hat{\bf x}) - f({\bf x})}
		\leq  \min{\set{\frac{1}{2}, d \left( \frac{1}{2} \right)^{d-1}}} \cdot \epsilon
		\end{align*}
	\end{lemma}

	\begin{proof}
        First we show for any $d$-dimensional vector ${\bf x} \in [0, \frac{1}{2}]^d$,
        \begin{align}
			\label{mv1pd}
			\sum_i^d \abs{\cfrac{\partial f({\bf x})}{\partial x_i}} \leq & \min{\set{\frac{1}{2}, d \left( \frac{1}{2} \right)^{d-1}}}
		\end{align}

    	For $d=0, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = 0$.

    	For $d=1, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{1}{\left( 2 - x_1 \right)^2} \leq \frac{4}{9} $.

    	For $d=2, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{x_1 + x_2}{\left( 2 - x_1x_2 \right)^2} \leq \frac{16}{49} $.

    	For $d=3, \sum_i\abs{ \cfrac{\partial f({\bf x})}{\partial x_i}} = \frac{x_1 x_2 + x_1 x_3 + x_2 x_3}{\left( 2 - x_1x_2x_3 \right)^2} \leq \frac{16}{75} $.

		Now denote $i^*$ be one of the indices of smallest $x_i$, since $x_i \leq \frac{1}{2}$, we have
    	\begin{align*}
    		\sum_i^d \abs{\cfrac{\partial f({\bf x})}{\partial x_i}}  =& \sum_i^d \cfrac{ \prod_{k \neq i}^d x_k  }{\left( 2 - \prod_i x_i \right)^2}
    		\leq  d \prod_{k \neq i^*}^d x_k
    		\leq  d \left( \frac{1}{2} \right)^{d-1}
    	\end{align*}

		So for $d \geq 4, \sum_i \abs{\cfrac{\partial f({\bf x})}{\partial x_i}} \leq \frac{1}{2}$, we have verified the inequality relation (\ref{mv1pd}).

        Now let $h(\alpha) = f(\alpha {\bf x} + (1 - \alpha) {\bf \hat{x}})$ where $\alpha \in [0,1]$, by (\ref{mv1pd}) and Mean Value Theorem, $\exists \tilde{\alpha} \in [0,1]$ s.t. for $\tilde{\bf x} = \tilde{\alpha} {\bf x} + (1 - \tilde{\alpha}) {\bf \hat{x}}$

		\begin{align*}
			\abs{f(\hat{\bf x}) - f({\bf x})} \leq& \sum_i\abs{ \frac{\partial f(\tilde{\bf x})}{\partial x_i}} \cdot \epsilon
		\leq  \min{\set{\frac{1}{2}, d \left( \frac{1}{2} \right)^{d-1}}} \cdot \epsilon
		\end{align*}
	\end{proof}

	\begin{lemma}
		\label{meanvalue2}
		Given estimated $\hat{\bf x},\hat{\bf y},\hat{\bf z}$ for true value $ {\bf x}, {\bf y}, {\bf z}$ respectively, such that ${\bf x} \in [0, \frac{1}{2}]^{d_1} , {\bf y,z} \in [0, \frac{1}{2}]^{d_2}, \hat{\bf x} \in [0, \frac{1}{2}]^{d_1} , \hat{\bf y},\hat{\bf z} \in [0, \frac{1}{2}]^{d_2}$,
		let $\epsilon \triangleq \max_i \set{\abs{x_i - \hat{x_i}}, \abs{y_i - \hat{y_i} } , \abs{z_i - \hat{z_i}}}$,
		\begin{align*}
		\abs{g(\hat{\bf x}, \hat{\bf y}, \hat{\bf z}) - g({\bf x,y,z})}
		\leq  3\epsilon
		\end{align*}

	\end{lemma}

	\begin{proof}
Denote

$S_1({\bf x,y,z}) \triangleq \sum_k^{d_1} \abs{ \frac{\partial g({\bf x,y,z})}{\partial x_k} }$,

$S_2({\bf x,y,z}) \triangleq \sum_k^{d_2} \abs{ \frac{\partial g({\bf x,y,z})}{\partial y_k} }$,

$S_3({\bf x,y,z}) \triangleq \sum_k^{d_2} \abs{ \frac{\partial g({\bf x,y,z})}{\partial z_k} }$,

$S_4({\bf x,y,z}) \triangleq S_1({\bf x,y,z})+S_2({\bf x,y,z})+S_3({\bf x,y,z})$.

let $X \triangleq \prod_i^{d_1} x_i$,
$Y \triangleq \prod_i^{d_2} y_i$,
$Z \triangleq \prod_i^{d_2} z_i$,
$W \triangleq (2+X Y-X-Z)^2$.

For ${\bf x} \in [0, \frac{1}{2}]^{d_1} , {\bf y,z} \in [0, \frac{1}{2}]^{d_2}$,
		\begin{align*}
		S_1({\bf x,y,z}) &= \frac{1}{W} \sum_k^{d_1} \prod_{i\neq k}^{d_1} x_i \cdot \left( 1 - Y \right)
		\leq \frac{d_1}{2^{d_1 - 1}}  \leq 1 \\
		S_2({\bf x,y,z}) &= \frac{1}{W} \sum_k^{d_2} \prod_{i\neq k}^{d_2} y_i \cdot X
		\leq  \frac{d_2}{2^{d_1 + d_2 - 1}} \leq 1 \\
		S_3({\bf x,y,z}) &= \frac{1}{W} \sum_k^{d_2} \prod_{i\neq k}^{d_2} z_i
		\leq \frac{d_2}{2^{d_2 - 1}} \leq 1
		\end{align*}

        Now let $h(\alpha) = g(\alpha {\bf x} + (1 - \alpha) {\bf \hat{x}}, \alpha {\bf y} + (1 - \alpha) {\bf \hat{y}}, \alpha {\bf z} + (1 - \alpha) {\bf \hat{z}})$ where $\alpha \in [0,1]$.

         By Mean Value Theorem, $\exists \tilde{\alpha} \in [0,1]$ s.t. for $\tilde{\bf x} = \tilde{\alpha} {\bf x} + (1 - \tilde{\alpha}) {\bf \hat{x}}, \tilde{\bf y} = \tilde{\alpha} {\bf y} + (1 - \tilde{\alpha}) {\bf \hat{y}}, \tilde{\bf z} = \tilde{\alpha} {\bf z} + (1 - \tilde{\alpha}) {\bf \hat{z}}$
        %Then by Mean Value Theorem, $\exists \tilde{\bf x}, \tilde{\bf y},  \tilde{\bf z}$ s.t. $\forall i, \min\set{x_i,\hat{x_i}} \leq \tilde{x_i} \leq \max\set{x_i, \hat{x_i}}, \min\set{y_i,\hat{y_i}} \leq \tilde{y_i} \leq \max\set{y_i, \hat{y_i}}, \min\set{z_i,\hat{z_i}} \leq \tilde{z_i} \leq \max\set{z_i, \hat{z_i}}$.
		\begin{align*}
		\abs{g(\hat{\bf x}, \hat{\bf y}, \hat{\bf z}) - g({\bf x,y,z})} \leq &
		S_4(\tilde{\bf x},\tilde{\bf y},\tilde{\bf z} ) \epsilon
		\leq  3\epsilon
		\end{align*}
	\end{proof}

	%Now we are ready for the proof of  Theorem \ref{cd-main-theorem}.

	%\begin{proof}

\noindent{\bf Proof of Theorem \ref{cd-main-theorem}. }
		Note that the recursion for normal edge case is applied only once, so it is sufficient to show that for free or dangling edge $e$:

		\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}\]
		
		And it automatically follows from Lemma \ref{meanvalue2} that for normal edge $e$:

		\[\abs{P(G,e,L) - P(G,e)} \leq 3\cdot(\frac{1}{2})^{L+1}\]

		Now we prove by induction with induction hypothesis that for free or dangling edge $e$:

		\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}\]
		
		For base case $L=0, \abs{P(G,e,L) - P(G,e)} \leq \frac{1}{2}$ holds when $e$ is free or dangling.

		Now suppose the induction hypothesis is true for $L<k$, we shall prove that it is true for $L=k$.

		{\bf Case 1}, $e$ is free edge $\abs{P(G,e,L) - P(G,e)} = 0$.

		{\bf Case 2}, $e=(u,\_)$ is dangling with $deg(u)=d+1$, then by induction hypothesis we have
		$\epsilon \triangleq \max_i \abs{P\left(G_i,e_i,L-\lceil \log_6{d+1}\rceil \right) - P(G_i,e_i)} \leq \left(\frac{1}{2}\right)^{L-\lceil \log_6{d+1}\rceil + 1}$.

		First by Lemma \ref{meanvalue1} we need to show that for $d \leq 4$,
        \[\frac{1}{2^{1+L-\lceil \log_6{(d+1)}\rceil + 1}} \leq \frac{1}{2^{L+1}}\]

		which is obvious because $\lceil\log_6{(d+1)}\rceil \leq 1$.

		Next we show for $d \geq 5$,
        \[ d\cdot \left( \frac{1}{2} \right)^{d-1 + L - \lceil \log_6{(d+1)}\rceil + 1}  \leq \left( \frac{1}{2} \right)^{L + 1} \]

		Namely for $d \geq 5$,
		\[ \log_2 d + \lceil \log_6{(d+1)} \rceil \leq d-1\]

		For $d=5,6$, one can directly examine that as $\log_2 d < 3$ and $\log_6 6 =1, \log_6 7 < 2$.

        %For $d\geq7$, by simply taking the derivative one can show that
		%\[ \log_2 d + \log_6{(d+1)} \leq d-1\]
		% @mrain:The derivative is wrong.
%        Since $\frac{d \log_2 x + \log_6 (x+1)}{dx} = \frac{1}{x} + \frac{1}{x+1} < 1$ when $x \geq 7$.
%        So for $d \ge 7$,

		For $d\geq 7$, since the function $f(x) = d-2 -\log_2 d - \log_6{(d+1)}$ is monotonically increasing, and $f(7)>0$, we have
        \[ \log_2 d + \log_6{(d+1)} + 1 \leq d-1\]

%		$\epsilon \leq \frac{1}{2}^{L - \lceil d/5 \rceil}$.
%		First we need to show that for $d \leq 5$,
%		\[\frac{1}{2^{1+L-\lceil d/5\rceil}} \leq \frac{1}{2^L}\]
%
%		which is obvious because $\lceil d/5 \rceil \leq 1$,
%
%		Next we show for $d \geq 6$,
%		\[ d\cdot \left( \frac{1}{2} \right)^{d-1 + L - \lceil d/5 \rceil}  \leq \left( \frac{1}{2} \right)^L \]
%
%		Namely for $d \geq 6$,
%		\[ \log_2 d + \lceil d/5 \rceil \leq d-1\]
%
%		This is followed from that the function $f(x) = x - 2 - d/5 - \log_2 d$ is monotonically increasing, and $f(6)>0$.

		Therefore, the hypothesis for $L=k$ is verified.
		%\[\abs{P(G,e,L) - P(G,e)} \leq (\frac{1}{2})^{L+1}, \textrm{for free or dangling edge $e$}\]

		To sum up, the case of free or dangling edge and the case of normal edge together conclude the proof for our main theorem.
	%\end{proof}
